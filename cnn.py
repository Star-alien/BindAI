# -*- coding: utf-8 -*-
"""CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1qqFm-AL_8eAuOR1ncxwCGP41bL7bHsnS
"""

from google.colab import drive

# Mount Google Drive to save the model later
drive.mount('/content/drive')

!pip install biopython

from Bio import Entrez, SeqIO

# Set your email to use the NCBI database
Entrez.email = "avanipersonal7@gmail.com"
api_key = "266c8d48bb34a15b47565d7ad34005122508"  # Your NCBI API key

# Retrieve a protein sequence by ID
def fetch_protein_sequence(protein_id):
    handle = Entrez.efetch(db="protein", id=protein_id, rettype="fasta", retmode="text", api_key=api_key)
    record = SeqIO.read(handle, "fasta")
    handle.close()
    return str(record.seq)

# Example protein ID
protein_sequence = fetch_protein_sequence("NP_000477.1")
print("Protein sequence:", protein_sequence)

import os
import numpy as np
import torch
from Bio import Entrez, SeqIO

# Set up Entrez with your email and API key
Entrez.email = "avanipersonal7@gmail.com"
api_key = "266c8d48bb34a15b47565d7ad34005122508"

# Directory to save the voxel data
voxel_data_dir = "/content/protein_voxel_data"
os.makedirs(voxel_data_dir, exist_ok=True)

# List of protein IDs (replace this with a real list of 2500 IDs)
protein_ids = ["NP_000013.1", "NP_000014.2", "NP_000015.1", "NP_000016.1", "NP_000017.2", "NP_000018.2", "NP_000019.2", "NP_000020.1", "NP_000021.2", "NP_000022.1",
"NP_000023.2", "NP_000024.1", "NP_000025.3", "NP_000026.2", "NP_000027.1", "NP_000028.1", "NP_000029.1", "NP_000030.1", "NP_000031.1", "NP_000032.2",
"NP_000033.2", "NP_000034.1", "NP_000035.1", "NP_000036.2", "NP_000037.1", "NP_000038.2", "NP_000039.2", "NP_000040.1", "NP_000041.2", "NP_000042.2",
"NP_000043.2", "NP_000044.1", "NP_000045.2", "NP_000046.1", "NP_000047.2", "NP_000048.2", "NP_000049.2", "NP_000050.2", "NP_000051.1", "NP_000052.1",
"NP_000053.1", "NP_000054.1", "NP_000055.2", "NP_000056.1", "NP_000057.1", "NP_000058.1", "NP_000059.1", "NP_000060.1", "NP_000061.1", "NP_000062.1",
"NP_000063.1", "NP_000064.1", "NP_000065.1", "NP_000066.1", "NP_000067.1", "NP_000068.1", "NP_000069.1", "NP_000070.1", "NP_000071.1", "NP_000072.1",
"NP_000073.1", "NP_000074.1", "NP_000075.1", "NP_000076.1", "NP_000077.1", "NP_000078.1", "NP_000079.1", "NP_000080.1", "NP_000081.1", "NP_000082.1",
"NP_000083.1", "NP_000084.1", "NP_000085.1", "NP_000086.1", "NP_000087.1", "NP_000088.1", "NP_000089.1", "NP_000090.1", "NP_000091.1", "NP_000092.1",
"NP_000093.1", "NP_000094.1", "NP_000095.1", "NP_000096.1", "NP_000097.1", "NP_000098.1", "NP_000099.1", "NP_000100.1", "NP_000101.1", "NP_000102.1",
"NP_000103.1", "NP_000104.1", "NP_000105.1", "NP_000106.1", "NP_000107.1", "NP_000108.1", "NP_000109.1", "NP_000110.1", "NP_000111.1", "NP_000112.1",
"NP_000113.1", "NP_000114.1", "NP_000115.1", "NP_000116.1", "NP_000117.1", "NP_000118.1", "NP_000119.1", "NP_000120.1", "NP_000121.1", "NP_000122.1",
"NP_000123.1", "NP_000124.1", "NP_000125.1", "NP_000126.1", "NP_000127.1", "NP_000128.1", "NP_000129.1", "NP_000130.1", "NP_000131.1", "NP_000132.1",
"NP_000133.1", "NP_000134.1", "NP_000135.1", "NP_000136.1", "NP_000137.1", "NP_000138.1", "NP_000139.1", "NP_000140.1", "NP_000141.1", "NP_000142.1",
"NP_000143.1", "NP_000144.1", "NP_000145.1", "NP_000146.1", "NP_000147.1", "NP_000148.1", "NP_000149.1", "NP_000150.1", "NP_000151.1", "NP_000152.1",
"NP_000153.1", "NP_000154.1", "NP_000155.1", "NP_000156.1", "NP_000157.1", "NP_000158.1", "NP_000159.1", "NP_000160.1", "NP_000161.1", "NP_000162.1",
"NP_000163.1", "NP_000164.1", "NP_000165.1", "NP_000166.1", "NP_000167.1", "NP_000168.1", "NP_000169.1", "NP_000170.1", "NP_000171.1", "NP_000172.1",
"NP_000173.1", "NP_000174.1", "NP_000175.1", "NP_000176.1", "NP_000177.1", "NP_000178.1", "NP_000179.1", "NP_000180.1", "NP_000181.1", "NP_000182.1",
"NP_000183.1", "NP_000184.1", "NP_000185.1", "NP_000186.1", "NP_000187.1", "NP_000188.1", "NP_000189.1", "NP_000190.1", "NP_000191.1", "NP_000192.1",
"NP_000193.1", "NP_000194.1", "NP_000195.1", "NP_000196.1", "NP_000197.1", "NP_000198.1", "NP_000199.1", "NP_000200.1"

]  # Add more protein IDs here

# Function to fetch protein sequences in batches
def fetch_protein_sequences(protein_ids, batch_size=100):
    sequences = []
    for start in range(0, len(protein_ids), batch_size):
        end = min(start + batch_size, len(protein_ids))
        batch_ids = protein_ids[start:end]
        print(f"Fetching batch {start}-{end}...")
        handle = Entrez.efetch(db="protein", id=",".join(batch_ids), rettype="fasta", retmode="text", api_key=api_key)
        batch_records = list(SeqIO.parse(handle, "fasta"))
        handle.close()
        sequences.extend(batch_records)
    return sequences

# Function to convert sequence to a voxel grid (simplified example)
def sequence_to_voxel(sequence, grid_size=(32, 32, 32)):
    grid = np.zeros(grid_size)
    for i, amino_acid in enumerate(sequence[:grid_size[0] * grid_size[1] * grid_size[2]]):
        x, y, z = i % grid_size[0], (i // grid_size[0]) % grid_size[1], (i // (grid_size[0] * grid_size[1])) % grid_size[2]
        grid[x, y, z] = ord(amino_acid) % 255 / 255.0  # Normalize to [0,1]
    return torch.tensor(grid).float().unsqueeze(0)  # Add channel dimension

# Function to save protein sequence as voxel data
def save_voxel_data(record, voxel_data_dir):
    sequence = str(record.seq)
    voxel_grid = sequence_to_voxel(sequence)
    label = torch.tensor(1.0)  # Placeholder label
    save_path = os.path.join(voxel_data_dir, f"{record.id}.pt")
    torch.save({'voxel': voxel_grid, 'label': label}, save_path)
    print(f"Saved voxel data for {record.id} at {save_path}")

# Fetch, process, and save voxel data
def create_protein_voxel_database(protein_ids):
    protein_sequences = fetch_protein_sequences(protein_ids)
    for record in protein_sequences:
        save_voxel_data(record, voxel_data_dir)

# Run the entire pipeline for the list of protein IDs
create_protein_voxel_database(protein_ids)

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import os

# Custom Dataset to load the voxel data
class ProteinVoxelDataset(Dataset):
    def __init__(self, data_dir):
        self.data_dir = data_dir
        self.files = os.listdir(data_dir)

    def __len__(self):
        return len(self.files)

    def __getitem__(self, idx):
        data = torch.load(os.path.join(self.data_dir, self.files[idx]))
        return data['voxel'], data['label']

# CNN model for protein voxel data
class ProteinVoxelCNN(nn.Module):
    def __init__(self):
        super(ProteinVoxelCNN, self).__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv3d(1, 32, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(32),
            nn.ReLU(),
            nn.MaxPool3d(2),

            nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(64),
            nn.ReLU(),
            nn.MaxPool3d(2),

            nn.Conv3d(64, 128, kernel_size=3, stride=1, padding=1),
            nn.BatchNorm3d(128),
            nn.ReLU(),
            nn.MaxPool3d(2)
        )

        self.fc_layers = nn.Sequential(
            nn.Linear(128 * 4 * 4 * 4, 256),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(256, 64),
            nn.ReLU(),
            nn.Dropout(0.4),
            nn.Linear(64, 1),  # Single output for binary classification (change if multiclass)
            nn.Sigmoid()  # For binary classification, replace with Softmax for multiclass
        )

    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)  # Flatten for fully connected layers
        x = self.fc_layers(x)
        return x

# Hyperparameters
batch_size = 16
learning_rate = 0.001
num_epochs = 20

# Dataset and DataLoader
data_dir = "/content/protein_voxel_data"
dataset = ProteinVoxelDataset(data_dir)
dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

# Model, loss function, and optimizer
model = ProteinVoxelCNN()
criterion = nn.BCELoss()  # Binary Cross-Entropy Loss for binary classification
optimizer = optim.Adam(model.parameters(), lr=learning_rate)

# Training loop
for epoch in range(num_epochs):
    model.train()
    running_loss = 0.0
    for i, (voxels, labels) in enumerate(dataloader):
        optimizer.zero_grad()
        outputs = model(voxels)
        loss = criterion(outputs, labels.unsqueeze(1))  # Match output shape for BCE Loss
        loss.backward()
        optimizer.step()
        running_loss += loss.item()

    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}")

print("Training complete.")

# Define the path within Google Drive
model_drive_path = "/content/drive/My Drive/protein_voxel_cnn_model.pth"

# Save the model to Google Drive
torch.save(model.state_dict(), model_drive_path)
print("Model saved successfully to Google Drive at", model_drive_path)

